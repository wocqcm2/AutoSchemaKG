#!/usr/bin/env python3
"""
ATLAS Multihop QA Benchmarking
åŸºäºatlas_multihopqa.ipynbæ”¹å†™çš„å®Œæ•´è„šæœ¬ç‰ˆæœ¬
"""

import os
import sys
import torch
import gc
import traceback
from configparser import ConfigParser
from openai import OpenAI
from atlas_rag.vectorstore.embedding_model import SentenceEmbedding
from sentence_transformers import SentenceTransformer
from atlas_rag.vectorstore import create_embeddings_and_index
from atlas_rag.evaluation import BenchMarkConfig
from atlas_rag import setup_logger
from atlas_rag.retriever import HippoRAG2Retriever
from atlas_rag.evaluation import RAGBenchmark
from atlas_rag.llm_generator import LLMGenerator

def setup_environment():
    """è®¾ç½®è¿è¡Œç¯å¢ƒ"""
    print("ğŸ”§ è®¾ç½®è¿è¡Œç¯å¢ƒ...")
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    
    # è®¾ç½®ä¸ºCPUæ¨¡å¼
    os.environ['CUDA_VISIBLE_DEVICES'] = ''
    print(f"ä½¿ç”¨è®¾å¤‡: CPU")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    # è®¾ç½®PyTorchçº¿ç¨‹æ•°ä»¥é¿å…å†…å­˜é—®é¢˜
    torch.set_num_threads(2)
    print(f"PyTorchçº¿ç¨‹æ•°: {torch.get_num_threads()}")

def setup_models():
    """è®¾ç½®ç¼–ç å™¨å’ŒLLMæ¨¡å‹"""
    print("\nğŸ¤– è®¾ç½®æ¨¡å‹...")
    
    try:
        # 1. è®¾ç½®ç¼–ç å™¨æ¨¡å‹
        encoder_model_name = 'all-MiniLM-L6-v2'
        sentence_model = SentenceTransformer(encoder_model_name, device='cpu')
        sentence_encoder = SentenceEmbedding(sentence_model)
        print(f"ç¼–ç å™¨æ¨¡å‹: {encoder_model_name}")
        print(f"æ¨¡å‹è®¾å¤‡: {sentence_model.device}")
        
        # 2. è®¾ç½®LLMæ¨¡å‹
        config = ConfigParser()
        config.read('config.ini')
        
        reader_model_name = "meta-llama/Meta-Llama-3.1-8B-Instruct"
        client = OpenAI(
            base_url="https://api.deepinfra.com/v1/openai",
            api_key=config['settings']['DEEPINFRA_API_KEY']
        )
        llm_generator = LLMGenerator(client=client, model_name=reader_model_name)
        print(f"LLMæ¨¡å‹: {reader_model_name}")
        print("APIç«¯ç‚¹: DeepInfra")
        
        return sentence_encoder, encoder_model_name, llm_generator, reader_model_name
    
    except Exception as e:
        print(f"âŒ æ¨¡å‹è®¾ç½®å¤±è´¥: {e}")
        raise

def load_data(sentence_encoder, encoder_model_name):
    """åŠ è½½å’Œå¤„ç†æ•°æ®"""
    print("\nğŸ“Š åŠ è½½æ•°æ®...")
    
    try:
        # ä½¿ç”¨ä¸æå–æ—¶ç›¸åŒçš„è®¾ç½®ï¼Œä½†å‡å°‘batch size
        keyword = 'musique_sample'
        working_directory = './working_data'
        
        print(f"æ•°æ®é›†: {keyword}")
        print(f"å·¥ä½œç›®å½•: {working_directory}")
        
        # ä½¿ç”¨æ›´å°çš„batch sizeä»¥å‡å°‘å†…å­˜ä½¿ç”¨
        data = create_embeddings_and_index(
            sentence_encoder=sentence_encoder,
            model_name=encoder_model_name,
            working_directory=working_directory,
            keyword=keyword,
            include_concept=True,
            include_events=True,
            normalize_embeddings=True,
            text_batch_size=8,  # å‡å°‘åˆ°8
            node_and_edge_batch_size=8,  # å‡å°‘åˆ°8
        )
        
        # æ¸…ç†å†…å­˜
        gc.collect()
        print("âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œå†…å­˜å·²æ¸…ç†")
        
        return data, keyword
    
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        print("ğŸ”§ å°è¯•ä½¿ç”¨æ›´å°çš„batch size...")
        traceback.print_exc()
        
        # å°è¯•ä½¿ç”¨æ›´å°çš„batch size
        try:
            data = create_embeddings_and_index(
                sentence_encoder=sentence_encoder,
                model_name=encoder_model_name,
                working_directory=working_directory,
                keyword=keyword,
                include_concept=True,
                include_events=True,
                normalize_embeddings=True,
                text_batch_size=4,  # è¿›ä¸€æ­¥å‡å°‘
                node_and_edge_batch_size=4,
            )
            gc.collect()
            return data, keyword
        except Exception as e2:
            print(f"âŒ å³ä½¿ä½¿ç”¨å°batch sizeä¹Ÿå¤±è´¥: {e2}")
            raise

def setup_benchmark(keyword, reader_model_name, encoder_model_name):
    """é…ç½®benchmarkæµ‹è¯•"""
    print("\nâš™ï¸ é…ç½®benchmark...")
    
    try:
        benchmark_config = BenchMarkConfig(
            dataset_name=keyword,
            question_file="benchmark_data/musique.json",
            include_concept=True,
            include_events=True,
            reader_model_name=reader_model_name,
            encoder_model_name=encoder_model_name,
            number_of_samples=3,  # å‡å°‘æ ·æœ¬æ•°é‡
        )
        
        logger = setup_logger(benchmark_config)
        return benchmark_config, logger
    
    except Exception as e:
        print(f"âŒ Benchmarké…ç½®å¤±è´¥: {e}")
        raise

def run_benchmark(data, llm_generator, sentence_encoder, benchmark_config, logger):
    """è¿è¡Œbenchmarkæµ‹è¯•"""
    print("\nğŸš€ å¼€å§‹benchmarkæµ‹è¯•...")
    
    try:
        # åˆå§‹åŒ–æ£€ç´¢å™¨
        hipporag2_retriever = HippoRAG2Retriever(
            llm_generator=llm_generator,
            sentence_encoder=sentence_encoder,
            data=data,
            logger=logger
        )
        
        # è¿è¡Œbenchmark
        benchmark = RAGBenchmark(config=benchmark_config, logger=logger)
        benchmark.run([hipporag2_retriever], llm_generator=llm_generator)
        
        # æ¸…ç†å†…å­˜
        gc.collect()
        
    except Exception as e:
        print(f"âŒ Benchmarkè¿è¡Œå¤±è´¥: {e}")
        traceback.print_exc()
        raise

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ”¥ ATLAS Multihop QA Benchmarking")
    print("=" * 60)
    
    try:
        # 1. è®¾ç½®ç¯å¢ƒ
        setup_environment()
        
        # 2. è®¾ç½®æ¨¡å‹
        sentence_encoder, encoder_model_name, llm_generator, reader_model_name = setup_models()
        
        # 3. åŠ è½½æ•°æ®
        data, keyword = load_data(sentence_encoder, encoder_model_name)
        
        # 4. è®¾ç½®benchmark
        benchmark_config, logger = setup_benchmark(keyword, reader_model_name, encoder_model_name)
        
        # 5. è¿è¡Œbenchmark
        run_benchmark(data, llm_generator, sentence_encoder, benchmark_config, logger)
        
        print("\nâœ… Benchmarkæµ‹è¯•å®Œæˆ!")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­äº†ç¨‹åº")
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main() 